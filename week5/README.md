## [Cilium Study 1ê¸°] 5ì£¼ì°¨ ì •ë¦¬
> ë³¸ ë‚´ìš©ì€ CloudNet@ Cilium Study 1ê¸° 5ì£¼ì°¨ ìŠ¤í„°ë””ì— ëŒ€í•œ ì •ë¦¬ ê¸€ì…ë‹ˆë‹¤. 

## Cilium BGP Control Plane
Cilium ë°°í¬
```sh
# BGPë¥¼ ì ìš©í•œ ì„¤ì • ë°°í¬
helm upgrade cilium cilium/cilium --version 1.18 --namespace kube-system \
--set k8sServiceHost=192.168.10.100 --set k8sServicePort=6443 \
--set ipam.mode="cluster-pool" --set ipam.operator.clusterPoolIPv4PodCIDRList={"172.20.0.0/16"} --set ipv4NativeRoutingCIDR=172.20.0.0/16 \
--set routingMode=native --set autoDirectNodeRoutes=false --set bgpControlPlane.enabled=true \
--set kubeProxyReplacement=true --set bpf.masquerade=true --set installNoConntrackIptablesRules=true \
--set endpointHealthChecking.enabled=false --set healthChecking=false \
--set hubble.enabled=true --set hubble.relay.enabled=true --set hubble.ui.enabled=true \
--set hubble.ui.service.type=NodePort --set hubble.ui.service.nodePort=30003 \
--set prometheus.enabled=true --set operator.prometheus.enabled=true --set hubble.metrics.enableOpenMetrics=true \
--set hubble.metrics.enabled="{dns,drop,tcp,flow,port-distribution,icmp,httpV2:exemplars=true;labelsContext=source_ip\,source_namespace\,source_workload\,destination_ip\,destination_namespace\,destination_workload\,traffic_direction}" \
--set operator.replicas=1 --set debug.enabled=true 

# cilium ì„¤ì • ì¡°íšŒ
cilium config view | grep -i bgp
bgp-router-id-allocation-ip-pool
bgp-router-id-allocation-mode                     default
bgp-secrets-namespace                             kube-system
enable-bgp-control-plane                          true
enable-bgp-control-plane-status-report            true
```

Sample Application ë°°í¬ 
```sh
# ìƒ˜í”Œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sample-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
---
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  nodeName: k8s-ctr
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# Application ìƒíƒœ ì¡°íšŒ
k get deploy,svc,ep webpod -owide
k get endpointslices -l app=webpod
k get ciliumendpoints 
NAME                      SECURITY IDENTITY   ENDPOINT STATE   IPV4           IPV6
curl-pod                  4455                ready            172.20.0.42
webpod-697b545f57-7qssc   47820               ready            172.20.2.193
webpod-697b545f57-87w4m   47820               ready            172.20.1.194
webpod-697b545f57-f2s6w   47820               ready            172.20.0.148

k get pod -o wide
NAME                      READY   STATUS    RESTARTS   AGE    IP             NODE      NOMINATED NODE   READINESS GATES
curl-pod                  1/1     Running   0          106m   172.20.0.42    k8s-ctr   <none>           <none>
webpod-697b545f57-7qssc   1/1     Running   0          106m   172.20.2.193   k8s-w0    <none>           <none>
webpod-697b545f57-87w4m   1/1     Running   0          106m   172.20.1.194   k8s-w1    <none>           <none>
webpod-697b545f57-f2s6w   1/1     Running   0          106m   172.20.0.148   k8s-ctr   <none>           <none>

# k8s-ctrì˜ ë¼ìš°íŠ¸ ì •ë³´
ip -c route 
172.20.0.0/24 via 172.20.0.53 dev cilium_host proto kernel src 172.20.0.53
172.20.0.0/16 via 192.168.10.200 dev eth1 proto static
172.20.0.53 dev cilium_host proto kernel scope link

# ë™ì¼í•œ ë…¸ë“œì˜ Podì— ëŒ€í•´ì„œë§Œ ì‘ë‹µì„ í•œë‹¤. ì™œëƒí•˜ë©´ autoDirectNodeRoutesë¥¼ falseë¡œ êµ¬ì„±í•˜ì—¬ ë¼ìš°íŒ… ì •ë³´ê°€ ì—†ê¸° ë•Œë¬¸ì´ë‹¤.
k exec -it curl-pod -- sh -c 'while true; do curl -s --connect-timeout 1 webpod | grep Hostname; echo "---" ; sleep 1; done'
---
---
---
Hostname: webpod-697b545f57-f2s6w
---
```

Ciliumì˜ [BGP](https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-v2/) ìš”ì†Œ
- `CiliumBGPClusterConfig`: Defines BGP instances and peer configurations that are applied to multiple nodes.
- `CiliumBGPPeerConfig`: A common set of BGP peering setting. It can be used across multiple peers.
- `CiliumBGPAdvertisement`: Defines prefixes that are injected into the BGP routing table.
- `CiliumBGPNodeConfigOverride`: Defines node-specific BGP configuration to provide a finer control.

```sh
vagrant ssh router 

ss -tnlp | grep -iE 'zebra|bgpd'
LISTEN 0      4096         0.0.0.0:179       0.0.0.0:*    users:(("bgpd",pid=4810,fd=22))
LISTEN 0      3          127.0.0.1:2605      0.0.0.0:*    users:(("bgpd",pid=4810,fd=18))
LISTEN 0      3          127.0.0.1:2601      0.0.0.0:*    users:(("zebra",pid=4805,fd=23))
LISTEN 0      4096            [::]:179          [::]:*    users:(("bgpd",pid=4810,fd=23))

ps -ef |grep frr
root        4792       1  0 03:10 ?        00:00:00 /usr/lib/frr/watchfrr -d -F traditional zebra bgpd staticd
frr         4805       1  0 03:10 ?        00:00:00 /usr/lib/frr/zebra -d -F traditional -A 127.0.0.1 -s 90000000
frr         4810       1  0 03:10 ?        00:00:00 /usr/lib/frr/bgpd -d -F traditional -A 127.0.0.1
frr         4817       1  0 03:10 ?        00:00:00 /usr/lib/frr/staticd -d -F traditional -A 127.0.0.1

# 
vtysh -c 'show running'
Current configuration:
!
frr version 8.4.4
frr defaults traditional
hostname router
log syslog informational
no ipv6 forwarding
service integrated-vtysh-config
!
router bgp 65000
 no bgp ebgp-requires-policy
 bgp graceful-restart
 bgp bestpath as-path multipath-relax
 !
 address-family ipv4 unicast
  network 10.10.1.0/24
  maximum-paths 4
 exit-address-family
exit
!
end

# frrì˜ bgp ì„¤ì • íŒŒì¼ ì¡°íšŒ
cat /etc/frr/frr.conf 
router bgp 65000
  bgp router-id
  bgp graceful-restart
  no bgp ebgp-requires-policy
  bgp bestpath as-path multipath-relax
  maximum-paths 4
  network 10.10.1.0/24

# BGPì— ëŒ€í•œ ì •ë³´ê°€ ì•„ì§ ì—†ë‹¤.
vtysh -c 'show ip bgp summary'
% No BGP neighbors found in VRF default

vtysh -c 'show ip bgp'
BGP table version is 1, local router ID is 192.168.20.200, vrf id 0 
Default local pref 100, local AS 65000
Status codes:  s suppressed, d damped, h history, * valid, > best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, < announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete
RPKI validation codes: V valid, I invalid, N Not found'

   Network          Next Hop            Metric LocPrf Weight Path
*> 10.10.1.0/24     0.0.0.0                  0         32768 i

Displayed  1 routes and 1 total paths

# router 
ip -c route
default via 10.0.2.2 dev eth0 proto dhcp src 10.0.2.15 metric 100
10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100
10.0.2.2 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100
10.0.2.3 dev eth0 proto dhcp scope link src 10.0.2.15 metric 100
10.10.1.0/24 dev loop1 proto kernel scope link src 10.10.1.200
10.10.2.0/24 dev loop2 proto kernel scope link src 10.10.2.200
192.168.10.0/24 dev eth1 proto kernel scope link src 192.168.10.200
192.168.20.0/24 dev eth2 proto kernel scope link src 192.168.20.200

vtysh -c 'show ip route'
K>* 0.0.0.0/0 [0/100] via 10.0.2.2, eth0, src 10.0.2.15, 00:03:27
C>* 10.0.2.0/24 [0/100] is directly connected, eth0, 00:03:27
K>* 10.0.2.2/32 [0/100] is directly connected, eth0, 00:03:27
K>* 10.0.2.3/32 [0/100] is directly connected, eth0, 00:03:27
C>* 10.10.1.0/24 is directly connected, loop1, 00:03:27
C>* 10.10.2.0/24 is directly connected, loop2, 00:03:27
C>* 192.168.10.0/24 is directly connected, eth1, 00:03:27
C>* 192.168.20.0/24 is directly connected, eth2, 00:03:27

# BGP ë„¤ì´ë²„ ìƒì„±
cat << EOF >> /etc/frr/frr.conf
  neighbor CILIUM peer-group
  neighbor CILIUM remote-as external
  neighbor 192.168.10.100 peer-group CILIUM
  neighbor 192.168.10.101 peer-group CILIUM
  neighbor 192.168.20.100 peer-group CILIUM 
EOF

# ì„¤ì • ê°’ ì¡°íšŒ
cat  /etc/frr/frr.conf

# ì„¤ì • ë¦¬ë¡œë“œ ë° ì¬ì‹œì‘
systemctl daemon-reexec && systemctl restart frr
systemctl status frr --no-pager --full

journalctl -u frr -f

# BGP ì „íŒŒí•  ë…¸ë“œ ë¼ë²¨ë§
k label nodes k8s-ctr k8s-w0 k8s-w1 enable-bgp=true

# BGP ë…¸ë“œ ì¡°íšŒ
k get node -l enable-bgp=true
k8s-ctr   Ready    control-plane   122m   v1.33.2
k8s-w0    Ready    <none>          118m   v1.33.2
k8s-w1    Ready    <none>          121m   v1.33.2

# BGP ë¦¬ì†ŒìŠ¤ ìƒì„± 
cat << EOF | kubectl apply -f -
apiVersion: cilium.io/v2
kind: CiliumBGPAdvertisement
metadata:
  name: bgp-advertisements
  labels:
    advertise: bgp
spec:
  advertisements:
    - advertisementType: "PodCIDR"
---
apiVersion: cilium.io/v2
kind: CiliumBGPPeerConfig
metadata:
  name: cilium-peer
spec:
  timers:
    holdTimeSeconds: 9
    keepAliveTimeSeconds: 3
  ebgpMultihop: 2
  gracefulRestart:
    enabled: true
    restartTimeSeconds: 15
  families:
    - afi: ipv4
      safi: unicast
      advertisements:
        matchLabels:
          advertise: "bgp"
---
apiVersion: cilium.io/v2
kind: CiliumBGPClusterConfig
metadata:
  name: cilium-bgp
spec:
  nodeSelector:
    matchLabels:
      "enable-bgp": "true"
  bgpInstances:
  - name: "instance-65001"
    localASN: 65001
    peers:
    - name: "tor-switch"
      peerASN: 65000
      peerAddress: 192.168.10.200  # router ip address
      peerConfigRef:
        name: "cilium-peer"
EOF

# ciliumì´ í˜„ì¬ ì—°ê²°ëœ ì„¸ì…˜ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ctr ë…¸ë‘ì˜ 51443 í¬íŠ¸ì™€ ë¼ìš°í„° ì„œë²„ì˜ 179ë‘ ì—°ê²°ì´ ë˜ì–´ìˆë‹¤.
ss -tnlp | grep 179
ss -tnp | grep 179
ESTAB 0      0               192.168.10.100:51443          192.168.10.200:179   users:(("cilium-agent",pid=5771,fd=50))

# BGP ìƒíƒœ ì¡°íšŒ
cilium bgp peers
Node      Local AS   Peer AS   Peer Address     Session State   Uptime   Family         Received   Advertised
k8s-ctr   65001      65000     192.168.10.200   established     33s      ipv4/unicast   4          2
k8s-w0    65001      65000     192.168.10.200   established     34s      ipv4/unicast   4          2
k8s-w1    65001      65000     192.168.10.200   established     34s      ipv4/unicast   4          2

cilium bgp routes available ipv4 unicast
Node      VRouter   Prefix          NextHop   Age   Attrs
k8s-ctr   65001     172.20.0.0/24   0.0.0.0   47s   [{Origin: i} {Nexthop: 0.0.0.0}]
k8s-w0    65001     172.20.2.0/24   0.0.0.0   47s   [{Origin: i} {Nexthop: 0.0.0.0}]
k8s-w1    65001     172.20.1.0/24   0.0.0.0   47s   [{Origin: i} {Nexthop: 0.0.0.0}]

k get ciliumbgpadvertisements,ciliumbgppeerconfigs,ciliumbgpclusterconfigs
NAME                                                  AGE
ciliumbgpadvertisement.cilium.io/bgp-advertisements   56s
NAME                                        AGE
ciliumbgppeerconfig.cilium.io/cilium-peer   56s
NAME                                          AGE
ciliumbgpclusterconfig.cilium.io/cilium-bgp   56s

k get ciliumbgpnodeconfigs -o yaml | yq |grep -i peering -A10
peeringState": "established",
                "routeCount": [
                  {
                    "advertised": 2,
                    "afi": "ipv4",
                    "received": 1,
                    "safi": "unicast"
                  }
                ]"

# Router
# bpgì— ëŒ€í•œ ì •ë³´ë¥¼ ìˆ˜ì‹ í–ˆë”°.
journalctl -u frr -f
Aug 17 03:14:18 router systemd[1]: Started frr.service - FRRouting.
Aug 17 03:16:38 router bgpd[5071]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.101 in vrf default
Aug 17 03:16:38 router bgpd[5071]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.20.100 in vrf default
Aug 17 03:16:38 router bgpd[5071]: [M59KS-A3ZXZ] bgp_update_receive: rcvd End-of-RIB for IPv4 Unicast from 192.168.10.100 in vrf default

# Router
ip -c route | grep bgp
172.20.0.0/24 nhid 32 via 192.168.10.100 dev eth1 proto bgp metric 20
172.20.1.0/24 nhid 30 via 192.168.10.101 dev eth1 proto bgp metric 20
172.20.2.0/24 nhid 31 via 192.168.20.100 dev eth2 proto bgp metric 20

# Router
vtysh -c 'show ip bgp summary'
BGP router identifier 192.168.20.200, local AS number 65000 vrf-id 0
Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc
192.168.10.100  4      65001        63        66        0    0    0 00:03:00            1        4 N/A
192.168.10.101  4      65001        63        66        0    0    0 00:03:00            1        4 N/A
192.168.20.100  4      65001        63        66        0    0    0 00:03:00            1        4 N/A

# Router
vtysh -c 'show ip bgp'
   Network          Next Hop            Metric LocPrf Weight Path
*> 10.10.1.0/24     0.0.0.0                  0         32768 i
*> 172.20.0.0/24    192.168.10.100                         0 65001 i
*> 172.20.1.0/24    192.168.10.101                         0 65001 i
*> 172.20.2.0/24    192.168.20.100                         0 65001 i


# íŒŒë“œê°„ì— í†µì‹  ì •ìƒ ì‘ë™ í™•ì¸
k exec -it curl-pod -- sh -c 'while true; do curl -s --connect-timeout 1 webpod | grep Hostname; echo "---" ; sleep 1; done'

# k8s-ctr 
tcpdump -i eth1 tcp port 179 -w /tmp/bgp.pcap

# routerì˜ frr ì¬ì‹œì‘
systemctl restart frr && journalctl -u frr -f

# í•„í„°ë§ bgp.type == 2.
# í•˜ì§€ë§Œ termsharkë¥¼ ì‚¬ìš©í•  ë•Œë§ˆë‹¤ k8s-ctrê°€ ì£½ëŠ”ë‹¤
termshark -r /tmp/bgp.pcap

# í™•ì¸ ëª»í–ˆìŒ
# cilium bgp routes
# ip -c route
```

ì¤‘ìš”í•œ ê²ƒì€ bgpê°€



## Kind
Kind ì„¤ì¹˜ ë° ìœ ìš©í•œ í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜
```sh
brew install kind
brew install kubernetes-cli
brew install krew
brew install kube-ps1
brew install kubectx
brew install helm
brew install kubecolor

# ì„¤ì¹˜ ì¡°íšŒ
kind --version
kubectl version --client=true
helm version

# ë‹¨ì¶•í‚¤ ì„¤ì • 
echo "alias kubectl=kubecolor" >> ~/.zshrc
echo "alias kubectl=kubecolor" >> ~/.zshrc
echo "compdef kubecolor=kubectl" >> ~/.zshrc
```

Cluster ë°°í¬
```sh
docker ps

kind create cluster --name myk8s --image kindest/node:v1.32.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000
    hostPort: 30000
  - containerPort: 30001
    hostPort: 30001
  - containerPort: 30002
    hostPort: 30002
  - containerPort: 30003
    hostPort: 30003
- role: worker
- role: worker
- role: worker
EOF

# ë…¸ë“œ ì¡°íšŒ
kind get nodes --name myk8s
kubens default

# kind ëŠ” ë³„ë„ ë„ì»¤ ë„¤íŠ¸ì›Œí¬ ìƒì„± í›„ ì‚¬ìš© : ê¸°ë³¸ê°’ 172.18.0.0/16
docker network ls
NETWORK ID     NAME            DRIVER    SCOPE
10d8011410ad   bridge          bridge    local
b832a580e3bf   host            host      local
e63730b2d612   kind            bridge    local

docker inspect kind | jq
    "Subnet": "172.19.0.0/16",
    "Gateway": "172.19.0.1"

# k8s api ì£¼ì†Œ í™•ì¸
k cluster-info
Kubernetes control plane is running at https://127.0.0.1:53890

# ë…¸ë“œ ì •ë³´ í™•ì¸
k get node -o wide
myk8s-control-plane   Ready    control-plane   3m19s   v1.32.2   172.19.0.4    <none>        Debian GNU/Linux 12 (bookworm)   6.10.14-linuxkit   containerd://2.0.3
myk8s-worker          Ready    <none>          3m10s   v1.32.2   172.19.0.2    <none>        Debian GNU/Linux 12 (bookworm)   6.10.14-linuxkit   containerd://2.0.3
...

# íŒŒë“œ ì •ë³´ í™•ì¸
k get pod -A -o wide
kube-system          kindnet-g8fjh                                 1/1     Running   0          3m41s   172.19.0.2   myk8s-worker          <none>           <none>
kube-system          kindnet-k8s2q                                 1/1     Running   0          3m40s   172.19.0.5   myk8s-worker2         <none>           <none>
kube-system          kindnet-mj6zn                                 1/1     Running   0          3m43s   172.19.0.4   myk8s-control-plane   <none>           <none>
kube-system          kindnet-wnxv4

k get ns
NAME                 STATUS   AGE
default              Active   4m19s
kube-node-lease      Active   4m19s
kube-public          Active   4m19s
kube-system          Active   4m19s
local-path-storage   Active   4m15s

docker ps
docker images
docker exec -it myk8s-control-plane ss -tnlp

# ë””ë²„ê·¸ìš© ë‚´ìš© ì¶œë ¥ì— ~/.kube/config ê¶Œí•œ ì¸ì¦ ë¡œë“œ
kubectl get pod -v6

# kubeconfig ì¡°íšŒ
cat ~/.kube/config

# í´ëŸ¬ìŠ¤í„° ì‚­ì œ
kind delete cluster --name myk8s
docker ps

# kubeconfig ì¡°íšŒ. kubeconfig ë‚´ìš©ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
cat ~/.kube/config
```

## Cluseter Mesh

Cluster ë°°í¬
```sh
# West í´ëŸ¬ìŠ¤í„° ë°°í¬
kind create cluster --name west --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30000 # sample apps
    hostPort: 30000
  - containerPort: 30001 # hubble ui
    hostPort: 30001
- role: worker
  extraPortMappings:
  - containerPort: 30002 # sample apps
    hostPort: 30002
networking:
  podSubnet: "10.0.0.0/16"
  serviceSubnet: "10.2.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# ì„¤ì¹˜ ë° ë…¸ë“œ ì¡°íšŒ
kubectl ctx
kind-west

k get node
NAME                 STATUS     ROLES           AGE    VERSION
west-control-plane   NotReady   control-plane   103s   v1.33.2
west-worker          NotReady   <none>          89s    v1.33.2

k get pods -A 

# ë…¸ë“œ ì—¬ëŸ¬ ë„êµ¬ ì„¤ì¹˜
docker exec -it west-control-plane sh -c 'apt update && apt install tree psmisc lsof wget net-tools dnsutils tcpdump ngrep iputils-ping git -y'
docker exec -it west-worker sh -c 'apt update && apt install tree psmisc lsof wget net-tools dnsutils tcpdump ngrep iputils-ping git -y'

# East ë°°í¬
kind create cluster --name east --image kindest/node:v1.33.2 --config - <<EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 31000 # sample apps
    hostPort: 31000
  - containerPort: 31001 # hubble ui
    hostPort: 31001
- role: worker
  extraPortMappings:
  - containerPort: 31002 # sample apps
    hostPort: 31002
networking:
  podSubnet: "10.1.0.0/16"
  serviceSubnet: "10.3.0.0/16"
  disableDefaultCNI: true
  kubeProxyMode: none
EOF

# kubeconfigë¥¼ í†µí•´ ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„° êµ¬ì„± í™•ì¸ 
k config get-contexts 
CURRENT   NAME        CLUSTER     AUTHINFO    NAMESPACE
*         kind-east   kind-east   kind-east
          kind-west   kind-west   kind-west
          minikube    minikube    minikube    default

# í´ëŸ¬ìŠ¤í„° ë³€ê²½
k config set-context kind-east

k get node -v=6 --context kind-east
NAME                 STATUS     ROLES           AGE   VERSION
east-control-plane   NotReady   control-plane   61s   v1.33.2
east-worker          NotReady   <none>          47s   v1.33.2

k get node -v=6
k get node -v=6 --context kind-west
NAME                 STATUS     ROLES           AGE     VERSION
west-control-plane   NotReady   control-plane   4m52s   v1.33.2
west-worker          NotReady   <none>          4m38s   v1.33.2

cat ~/.kube/config

kubectl get pod -A
kubectl get pod -A --context kind-west

# ë…¸ë“œ ì—¬ëŸ¬ ë„êµ¬ ì„¤ì¹˜
docker exec -it east-control-plane sh -c 'apt update && apt install tree psmisc lsof wget net-tools dnsutils tcpdump ngrep iputils-ping git -y'
docker exec -it east-worker sh -c 'apt update && apt install tree psmisc lsof wget net-tools dnsutils tcpdump ngrep iputils-ping git -y'

# ë‹¨ì¶•í‚¤ ì§€ì •
alias kwest='kubectl --context kind-west'
alias keast='kubectl --context kind-east'

kwest get node -owide
keast get node -owide
```

Cilum ë°°í¬
```sh
brew install cilium-cli

# helm ì„¤ì • ì¶œë ¥. ì‹¤ì œë¡œ ë°°í¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
cilium install --version 1.17.6 --set ipam.mode=kubernetes \
--set kubeProxyReplacement=true --set bpf.masquerade=true \
--set endpointHealthChecking.enabled=false --set healthChecking=false \
--set operator.replicas=1 --set debug.enabled=true \
--set routingMode=native --set autoDirectNodeRoutes=true --set ipv4NativeRoutingCIDR=10.0.0.0/16 \
--set ipMasqAgent.enabled=true --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.1.0.0/16}' \
--set cluster.name=west --set cluster.id=1 \
--context kind-west --dry-run-helm-values

# west ë°°í¬
# clusetì˜ ì´ë¦„ê³¼ ê³ ìœ ê°’ì„ ì§€ì •í•œë‹¤.
cilium install --version 1.17.6 --set ipam.mode=kubernetes \
--set kubeProxyReplacement=true --set bpf.masquerade=true \
--set endpointHealthChecking.enabled=false --set healthChecking=false \
--set operator.replicas=1 --set debug.enabled=true \
--set routingMode=native --set autoDirectNodeRoutes=true --set ipv4NativeRoutingCIDR=10.0.0.0/16 \
--set ipMasqAgent.enabled=true --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.1.0.0/16}' \
--set cluster.name=west --set cluster.id=1 \
--context kind-west

# east
cilium install --version 1.17.6 --set ipam.mode=kubernetes \
--set kubeProxyReplacement=true --set bpf.masquerade=true \
--set endpointHealthChecking.enabled=false --set healthChecking=false \
--set operator.replicas=1 --set debug.enabled=true \
--set routingMode=native --set autoDirectNodeRoutes=true --set ipv4NativeRoutingCIDR=10.1.0.0/16 \
--set ipMasqAgent.enabled=true --set ipMasqAgent.config.nonMasqueradeCIDRs='{10.0.0.0/16}' \
--set cluster.name=east --set cluster.id=2 \
--context kind-east

# íŒŒë“œ ìƒíƒœ ì¡°íšŒ. cilium ë°°í¬ë¡œ ëª¨ë“  íŒŒë“œê°€ Running ìƒíƒœì´ë‹¤
kwest get pod -A && keast get pod -A
kube-system          kube-controller-manager-west-control-plane   1/1     Running   0          8m58s
kube-system          kube-scheduler-west-control-plane            1/1     Running   0          8m58s
local-path-storage   local-path-provisioner-7dc846544d-qgxn4      1/1     Running   0          8m51s
NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE
kube-system          cilium-cxvbg                                 1/1     Running   0          54s
kube-system          cilium-envoy-kwtg2                           1/1     Running   0          54s

# cluster ë³„ cilium ìƒíƒœ ì¡°íšŒ
cilium status --context kind-east
cilium status --context kind-west

# cluster ë³„ ì„¤ì • ì¡°íšŒ
cilium config view --context kind-west
cilium config view --context kind-east
kwest exec -it -n kube-system ds/cilium -- cilium status --verbose
keast exec -it -n kube-system ds/cilium -- cilium status --verbose

# íŒŒë“œ CIDR ì¡°íšŒ
kwest -n kube-system exec ds/cilium -c cilium-agent -- cilium-dbg bpf ipmasq list
IP PREFIX/ADDRESS
10.1.0.0/16
169.254.0.0/16

# íŒŒë“œ CIDR ì¡°íšŒ
keast -n kube-system exec ds/cilium -c cilium-agent -- cilium-dbg bpf ipmasq list
IP PREFIX/ADDRESS
10.0.0.0/16
169.254.0.0/16

# ë‘ í´ëŸ¬ìŠ¤í„° ëª¨ë‘ ê¸°ë³¸ ë„ë©”ì¸ì„ ì‚¬ìš©í•œë‹¤
kubectl describe cm -n kube-system coredns --context kind-west | grep kubernetes
    kubernetes cluster.local in-addr.arpa ip6.arpa {
kubectl describe cm -n kube-system coredns --context kind-west | grep kubernetes
    kubernetes cluster.local in-addr.arpa ip6.arpa {

# k9sì˜ ê²½ìš°ë„ ë™ì¼í•œ ì˜µì…˜ ì‚¬ìš©
k9s --context kind-west
k9s --context kind-east
```

CluseterMesh êµ¬ì„±
```sh
# ì‹ ê¸°í•œ ì ì€ ClusterMeshì„ native ë¼ìš°íŒ…ê³¼ ê°™ì€ ë„¤íŠ¸ì›Œí¬ ëŒ€ì¸ ê²½ìš° ìë™ìœ¼ë¡œ ë¼ìš°íŒ…ì„ ì£¼ì…í•œë‹¤. 
docker exec -it west-control-plane ip -c route
docker exec -it west-worker ip -c route
docker exec -it east-control-plane ip -c route
docker exec -it east-worker ip -c route

# cilium cluster name ë° id ì¡°íšŒ
cilium config view --context kind-west |grep cluster-
cluster-id                                        1
cluster-name                                      west

cilium config view --context kind-east |grep cluster-
cluster-id                                        2
cluster-name                                      east


# ciliumê°„ì— cluster meshë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•´ ë™ì¼í•œ caë¥¼ ì‚¬ìš©í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ íŠ¹ì • cilium caë¥¼ ì‚­ì œ í›„ ë‚˜ë¨¸ì§€ í´ëŸ¬ìŠ¤í„°ì˜ cilium caë¥¼ ë³µì œí•œë‹¤.
keast get secret -n kube-system cilium-ca
keast delete secret -n kube-system cilium-ca

kubectl --context kind-west get secret -n kube-system cilium-ca -o yaml | \
kubectl --context kind-east create -f -

keast get secret -n kube-system cilium-ca
cilium-ca   Opaque   2      4s

cilium clustermesh status --context kind-west --wait  
cilium clustermesh status --context kind-east --wait
âŒ› Waiting (0s) for access information: unable to get clustermesh service "clustermesh-apiserver": services "clustermesh-apiserver" not found

# cluster meshë¥¼ Nodeport íƒ€ì…ì˜ ì„œë¹„ìŠ¤ë¡œ í™œì„±í™”í•œë‹¤. ê¶Œì¥í•˜ëŠ” íƒ€ì…ì˜ ë¡œë“œë°¸ëŸ°ì„œì´ë‹¤.
cilium clustermesh enable --service-type NodePort --enable-kvstoremesh=false --context kind-west
cilium clustermesh enable --service-type NodePort --enable-kvstoremesh=false --context kind-east

# clustermesh-apiserverë¥¼ ìœ„í•œ íŒŒë“œê°€ ìƒì„±ë˜ì—ˆë‹¤.
kwest get pod -n kube-system -owide | grep clustermesh
clustermesh-apiserver-5cf45db9cc-w9hjt       2/2     Running     0          62s   10.0.1.230   west-worker          <none>           <none>
clustermesh-apiserver-generate-certs-t2kxt   0/1     Completed   0          62s   172.19.0.2   west-worker          <none>           <none>

kwest get svc,ep -n kube-system clustermesh-apiserver --context kind-west
NAME                            TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
service/clustermesh-apiserver   NodePort   10.2.10.77   <none>        2379:32379/TCP   43s
NAME                              ENDPOINTS         AGE
endpoints/clustermesh-apiserver   10.0.1.230:2379   43s

# í´ëŸ¬ìŠ¤í„° ì—°ë™ 
cilium clustermesh connect --context kind-west --destination-context kind-east

# ì—°ë™ ì¡°íšŒ
cilium clustermesh status --context kind-east --wait
cilium clustermesh status --context kind-west --wait
âš ï¸  Service type NodePort detected! Service may fail when nodes are removed from the cluster!
âœ… Service "clustermesh-apiserver" of type "NodePort" found
âœ… Cluster access information is available:
  - 172.19.0.3:32379
âœ… Deployment clustermesh-apiserver is ready
â„¹ï¸  KVStoreMesh is disabled

âœ… All 2 nodes are connected to all clusters [min:1 / avg:1.0 / max:1]

ğŸ”Œ Cluster Connections:
  - east: 2/2 configured, 2/2 connected

ğŸ”€ Global services: [ min:0 / avg:0.0 / max:0 ]

# ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•œ ì •ë³´ ê·¸ë¦¬ê³  TLS, ETCDì— ëŒ€í•œ ì •ë³´ë¥¼ ì¶œë ¥í•œë‹¤.cilium status --context kind-west
kubectl exec -it -n kube-system ds/cilium -c cilium-agent --context kind-west -- cilium-dbg troubleshoot clustermesh
kubectl exec -it -n kube-system ds/cilium -c cilium-agent --context kind-east -- cilium-dbg troubleshoot clustermesh

cilium status --context kind-west
Deployment             clustermesh-apiserver    Desired: 1, Ready: 1/1, Available: 1/1
Cluster Pods:          4/4 managed by Cilium
...

keast exec -it -n kube-system ds/cilium -- cilium status --verbose
ClusterMesh:   1/1 remote clusters ready, 0 global-services
   west: ready, 2 nodes, 4 endpoints, 3 identities, 0 services, 0 MCS-API service exports, 0 reconnections (last: never)
   â””  etcd: 1/1 connected, leases=0, lock leases=0, has-quorum=true: endpoint status checks are disabled, ID: 9f649a615d34326f
   â””  remote configuration: expected=true, retrieved=true, cluster-id=1, kvstoremesh=false, sync-canaries=true, service-exports=disabled

# helmì— ëŒ€í•œ ê°’ ì¶œë ¥ 
helm get values -n kube-system cilium --kube-context kind-west 
cluster:
  id: 1
  name: west
clustermesh:
  apiserver:
    kvstoremesh:
      enabled: false
    service:
      type: NodePort
    tls:
      auto:
        enabled: true
        method: cronJob
        schedule: 0 0 1 */4 *
  config:
    clusters:
    - ips:
      - 172.19.0.4
      name: east
      port: 32379
    enabled: true
  useAPIServer: true


# ë¼ìš°íŒ… ì •ë³´ ì¡°íšŒ ì‹œ ë‘ í´ëŸ¬ìŠ¤í„°ê°„ì— ê° ë…¸ë“œì— ëŒ€í•œ ë¼ìš°íŠ¸ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë”°.
docker exec -it west-control-plane ip -c route
docker exec -it west-worker ip -c route
docker exec -it east-control-plane ip -c route
docker exec -it east-worker ip -c route

# í—ˆë¸” í™œì„±í™”
cilium hubble enable --ui --relay --context kind-west
cilium hubble enable --ui --relay --context kind-east
```

Sample Application ë°°í¬
```sh
cat << EOF | kubectl apply --context kind-west -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

cat << EOF | kubectl apply --context kind-east -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

# íŒŒë“œ ip ì¡°íšŒ
kwest get pod -owide && keast get pod -owide
NAME       READY   STATUS    RESTARTS   AGE   IP           NODE          NOMINATED NODE   READINESS GATES
curl-pod   1/1     Running   0          53s   10.0.1.188   west-worker   <none>           <none>
NAME       READY   STATUS    RESTARTS   AGE   IP          NODE          NOMINATED NODE   READINESS GATES
curl-pod   1/1     Running   0          53s   10.1.1.81   east-worker   <none>           <none>

# ë‘ í´ëŸ¬ìŠ¤í„° ê°„ì— icmp ì‘ë‹µ í™•ì¸
kubectl exec -it curl-pod --context kind-west -- ping -c 1 10.1.1.81
kubectl exec -it curl-pod --context kind-west -- ping 10.0.1.188

# ë‘ í´ëŸ¬ìŠ¤í„° ê°„ì— ì–´ë– í•œ NAT ì²˜ë¦¬ ì—†ì´ íŒŒë“œë¡œ ë°”ë¡œ í†µì‹ í•˜ëŠ” ê²ƒì„ í™•ì¸í• ìˆ˜ ìˆë‹¤.
kubectl exec -it curl-pod --context kind-east -- tcpdump -i eth0 -nn
kubectl exec -it curl-pod --context kind-east -- ping -c 1 10.0.1.188
19:14:19.801251 IP 10.1.1.81 > 10.0.1.188: ICMP echo request, id 3, seq 1, length 64
19:14:19.801432 IP 10.0.1.188 > 10.1.1.81: ICMP echo reply, id 3, seq 1, length 64


# ì„œë¹„ìŠ¤ ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± 
cat << EOF | kubectl apply --context kind-west -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sample-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
  annotations:
    service.cilium.io/global: "true"
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
EOF

cat << EOF | kubectl apply --context kind-east -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webpod
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webpod
  template:
    metadata:
      labels:
        app: webpod
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - sample-app
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: webpod
        image: traefik/whoami
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: webpod
  labels:
    app: webpod
  annotations:
    service.cilium.io/global: "true"
spec:
  selector:
    app: webpod
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  type: ClusterIP
EOF

kwest get svc,ep webpod && keast get svc,ep webpod

# ë§Œì¼ í˜„ì¬ k8s cluseterì— íŒŒë“œê°€ ì—†ë”ë¼ë„ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì˜ íŒŒë“œë¡œ ìš”ì²­ì„ ë³´ë‚¼ìˆ˜ê°€ ìˆë‹¤.
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
9    10.2.183.236:80/TCP    ClusterIP      1 => 10.0.1.155:80/TCP (active)
                                           2 => 10.0.1.117:80/TCP (active)
                                           3 => 10.1.1.179:80/TCP (active)
                                           4 => 10.1.1.27:80/TCP (active)

# globalìœ¼ë¡œ ì„¤ì •ì´ ë˜ì–´ìˆê¸° ë•Œë¬¸ì´ë‹¤.
kwest describe svc webpod | grep Annotations -A1
Annotations:              service.cilium.io/global: true
Selector:                 app=webpod

# ìš”ì²­ ë°˜ë³µ 
kubectl exec -it curl-pod --context kind-west -- sh -c 'while true; do curl -s --connect-timeout 1 webpod ; sleep 1; echo "---"; done;'
kubectl exec -it curl-pod --context kind-east -- sh -c 'while true; do curl -s --connect-timeout 1 webpod ; sleep 1; echo "---"; done;'

# ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì˜ íŒŒë“œë¡œ í†µì‹ ì´ ê°„ë‹¤.
kwest scale deployment webpod --replicas 0
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
9    10.2.183.236:80/TCP    ClusterIP      1 => 10.1.1.179:80/TCP (active)
                                           2 => 10.1.1.27:80/TCP (active)



# í˜„ì¬ k8s í´ëŸ¬ìŠ¤í„°ë¥¼ ìš°ì„ ìœ¼ë¡œ í•˜ì—¬ ìš”ì²­ì„ ì „ë‹¬í•œë‹¤. ë§ì¸ ì¦‰ìŠ¨ í˜„ì¬ í´ëŸ¬ìŠ¤í„°ì—ì„œ íŒŒë“œê°€ ì—†ì„ ê²½ìš°ì—ëŠ” ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ë¡œ ìš”ì²­ì„ ì „ë‹¬í•˜ê²Œ ëœë‹¤.
kwest annotate service webpod service.cilium.io/affinity=local --overwrite
kwest describe svc webpod | grep Annotations -A3

kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
9    10.2.183.236:80/TCP    ClusterIP      1 => 10.1.1.179:80/TCP (active)
                                           2 => 10.1.1.27:80/TCP (active)
                                           3 => 10.0.1.234:80/TCP (active) (preferred)
                                           4 => 10.0.1.128:80/TCP (active) (preferred)

kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
9    10.2.183.236:80/TCP    ClusterIP      1 => 10.1.1.179:80/TCP (active)
                                           2 => 10.1.1.27:80/TCP (active)


# localê³¼ ë°˜ëŒ€ë˜ëŠ” ê°œë…ìœ¼ë¡œ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ë¥¼ ìš°ì„ í•œë‹¤.
kwest annotate service webpod service.cilium.io/affinity=remote --overwrite
kwest describe svc webpod | grep Annotations -A3
Annotations:              service.cilium.io/affinity: remote
                          service.cilium.io/global: true

kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
9    10.2.183.236:80/TCP    ClusterIP      1 => 10.1.1.179:80/TCP (active) (preferred)
                                           2 => 10.1.1.27:80/TCP (active) (preferred)
                                           3 => 10.0.1.77:80/TCP (active)
                                           4 => 10.0.1.253:80/TCP (active)



# ìƒíƒœ ì›ë³µ 
kwest annotate service webpod service.cilium.io/affinity=local --overwrite
keast annotate service webpod service.cilium.io/affinity=local --overwrite

kest describe svc webpod | grep Annotations -A3
Annotations:              service.cilium.io/affinity: local
                          service.cilium.io/global: true

# sharedë¥¼ falseë¡œ í•˜ë©´ íŒŒë“œ ipì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì— ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤.
kwest annotate service webpod service.cilium.io/shared=false
service/webpod annotated

kwest describe svc webpod | grep Annotations -A3
Annotations:              service.cilium.io/affinity: local
                          service.cilium.io/global: true
                          service.cilium.io/shared: false
Selector:                 app=webpod

# west í´ëŸ¬ìŠ¤í„°ëŠ” 4ê°œì˜ íŒŒë“œì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ, eastëŠ” 2ê°œì˜ íŒŒë“œ ì •ë³´ë§Œì„ ì¶œë ¥í•œë‹¤.
kwest exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
keast exec -it -n kube-system ds/cilium -c cilium-agent -- cilium service list --clustermesh-affinity
9    10.2.183.236:80/TCP    ClusterIP      1 => 10.1.1.179:80/TCP (active)
                                           2 => 10.1.1.27:80/TCP (active)
                                           3 => 10.0.1.77:80/TCP (active) (preferred)
                                           4 => 10.0.1.253:80/TCP (active) (preferred)

11   10.3.191.80:80/TCP      ClusterIP      1 => 10.1.1.179:80/TCP (active) (preferred)
                                            2 => 10.1.1.27:80/TCP (active) (preferred)
```


## krew [pexec](https://github.com/ssup2/kpexec)
íŒŒë“œë¥¼ ìƒì„±í•˜ê²Œ ë˜ë©´ ë³´ì•ˆì ì¸ ì´ìœ ë¡œ bashë¥¼ ì œê±°í•œë‹¤. í•˜ì§€ë§Œ í•´ë‹¹ í”ŒëŸ¬ê·¸ì¸ì„ í†µí•´ ìš°íšŒí•´ì„œ bashë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```sh
brew install ssup2/tap/kpexec

k exec -it -n kube-system clustermesh-apiserver-5cf45db9cc-hsqj5 -- bash
Defaulted container "etcd" out of: etcd, apiserver, etcd-init (init)
error: Internal error occurred: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec "01c9b126fb9c2720a9d6c662060687a7c57279d0431f2f85cdf36eb6bbdfb260": OCI runtime exec failed: exec failed: unable to start container process: exec: "bash": executable file not found in $PATH

kubectl get pod -n kube-system -l k8s-app=clustermesh-apiserver
DPOD=clustermesh-apiserver-5cf45db9cc-h2vtp

# íŒŒë“œì—ì„œ ì‹¤í–‰ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì •ë³´, ì‹¤í–‰ì‹œ ì£¼ì…ëœ ì„¤ì • ë° í¬íŠ¸ë“± ë‹¤ì–‘í•œ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 
kubectl pexec clustermesh-apiserver-5cf45db9cc-hsqj5 -it -T -n kube-system -c etcd -- bash
ps -ef -T -o pid,ppid,comm,args
ps -ef -T -o args
cat /proc/1/cmdline ; echo
ss -tnlp
ss -tnp

kubectl pexec $DPOD -it -T -n kube-system -c apiserver -- bash
ps -ef -T -o pid,ppid,comm,args

# í´ëŸ¬ìŠ¤í„° ì‚­ì œ 
kind delete cluster --name west && kind delete cluster --name east && docker rm -f mypc
```

